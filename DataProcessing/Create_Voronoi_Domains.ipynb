{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f164bdfd-3856-4191-acc5-9df9b0b61c01",
   "metadata": {},
   "source": [
    "If you have the Kernel Metric Network, you can use it to creat Reaction Specific Fingerprint.\n",
    "\n",
    "From these fingerprints, you can divide them into Voronoi reagions using FAISS.\n",
    "\n",
    "Lastly, record the Voronoi/Expert indices to the Pistachio Dataframe to train individual experts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d35d3-b73f-47f1-bc83-8a885ec992cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.special import softmax\n",
    "\n",
    "    \n",
    "class KernelMetricNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(KernelMetricNetwork, self).__init__()\n",
    "        print('Using', num_classes, 'classes predictions')\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(256)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.batch_norm1(self.relu(self.fc1(x))))\n",
    "        x = self.dropout(self.batch_norm2(self.relu(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "    def get_embeddings(self,x):\n",
    "        \n",
    "        x = self.batch_norm1(self.relu(self.fc1(x)))\n",
    "        x = self.batch_norm2(self.relu(self.fc2(x)))\n",
    "        \n",
    "        return(x)\n",
    "\n",
    "def load_model(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    return model    \n",
    "\n",
    "model = load_model(KernelMetricNetwork(2048*3, 2285), \"best_model_50ep_4096batchsize_AdamW.pth\")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919d486-58ae-4c63-8a33-e4103e0abc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "from rdkit.Chem import AllChem,DataStructs\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import faiss\n",
    "from typing import List,Tuple\n",
    "from rdkit import RDLogger     \n",
    "from faiss import write_index, read_index\n",
    "RDLogger.DisableLog('rdApp.*')   \n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eac9553-5fb8-4981-abb7-cda248f39014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Both of these two fingerprint can be efficiently generated using multi-processing from the Get_Molecukar_FP_MultiProcessing_LargeRAM.ipynb notebook.\n",
    "\n",
    "# This variable below contains the array of features used to train the FAISS clustering of voronoi reagions\n",
    "mix_fp_features = pickle.load(open('/global/cfs/cdirs/m410/haoteli/LLaMA/Mixture_Expert_Prediction_Preparation/MixFP_Reactant_Features_p4_r2_update_1024_dim.pkl', 'rb'))\n",
    "\n",
    "# This variable below contains the array of features that is used to train the Llama3.1 model.\n",
    "mix_fp_features_to_add = pickle.load(open('/global/cfs/cdirs/m410/haoteli/LLaMA/Mixture_Expert_Prediction_Preparation/MixFP_Train_Xprt_Reactant_Features_p4_r2_update_1024_dim.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bdef64-f37f-40f2-9f2e-3b12a2ef4631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# processing all (concated/mix)-fingerprints into RSFP\n",
    "# The following is executed on a single A100 on a shared node with 128GB RAM.\n",
    "\n",
    "def process_features_in_batches(model, mix_fp_features, batch_size=512, device=device):\n",
    "    # Convert the input list/array to a numpy array if it isn't already\n",
    "    if not isinstance(mix_fp_features, np.ndarray):\n",
    "        mix_fp_features = np.array(mix_fp_features)\n",
    "    \n",
    "    # Calculate the number of batches\n",
    "    n_samples = len(mix_fp_features)\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Initialize list to store features\n",
    "    features = []\n",
    "    \n",
    "    # Process in batches\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in tqdm(range(n_batches)):\n",
    "            # Get batch indices\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = min((i + 1) * batch_size, n_samples)\n",
    "            \n",
    "            # Prepare batch\n",
    "            batch = mix_fp_features[start_idx:end_idx]\n",
    "            batch_tensor = torch.from_numpy(batch).float().to(device)\n",
    "            \n",
    "            # Get embeddings for batch\n",
    "            batch_features = model.get_embeddings(batch_tensor)\n",
    "            #batch_features = model(batch_tensor)\n",
    "            \n",
    "            # Store results\n",
    "            features.extend(batch_features.cpu().numpy())\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Usage example:\n",
    "batch_size = 1024  # Adjust based on your GPU memory and model size\n",
    "features = process_features_in_batches(model, mix_fp_features, batch_size=batch_size)\n",
    "# The training features for FAISS\n",
    "\n",
    "xprt_features = process_features_in_batches(model, mix_fp_features_to_add, batch_size=batch_size)\n",
    "# The training features for Llama3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a3d15d-d31b-42f6-b158-ff75ed30b9f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = features.shape[-1]\n",
    "\n",
    "nlist = 2500  # how many cells\n",
    "print('nlist =',nlist)\n",
    "quantizer = faiss.IndexFlatL2(d)\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist)\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "print('Converting to GPU index')\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index) # comment out this line if you do not, or dont have a GPU.\n",
    "\n",
    "\n",
    "print('Training FAISS')\n",
    "index.train(features)                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
    "print('Finished Training')\n",
    "assert index.is_trained  # This has to be True, otherwise something is wrong\n",
    "print('Adding features')\n",
    "#index.add(features)\n",
    "index.add(xprt_features)\n",
    "print('Finished adding features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834918c4-57e5-472e-82b7-8f2877b2bc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cpu_index = faiss.index_gpu_to_cpu(index) # converting back to CPU index to save it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcaab9-e108-434d-9bc5-fde7a399903c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Loading the pistachio database and sub-indexing the relevant files. This will then be added the voronoi indices for later training\n",
    "\n",
    "df = pickle.load(open('/global/cfs/cdirs/m410/haoteli/EnLatent_Challenge/DataReaction_Match/FPCompatible_Cleaned_Pistachio.pkl','rb'))\n",
    "\n",
    "df = df[~df['paragraphText'].isna()].reset_index(drop=True) # If there are no descriptions, drop them\n",
    "df = df[~(df['agent'] == '[]')].reset_index(drop=True) # If there are no yield, drop them\n",
    "df = df[~(df['agent_name'] == '[]')].reset_index(drop=True) # If there are no yield, drop them\n",
    "df = df[~(df['solvent'] == '[]')].reset_index(drop=True)\n",
    "df = df[~(df['solvent_name'] == '[]')].reset_index(drop=True) # If there are no yield, drop them\n",
    "df = df[~(df['yield'] == '[]')].reset_index(drop=True)\n",
    "df = df[['Example' not in p for p in df['paragraphText']] ].reset_index(drop=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1258a07-fc6a-48da-ad49-fce05049dead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculating the Voronoi index assignments from FAISS\n",
    "print('Calculating Dataset Quadrants')\n",
    "distance, cell_ids = index.quantizer.search(xprt_features.reshape((-1,d)), k=1)\n",
    "cell_ids = cell_ids.flatten()\n",
    "print('Adding Expert IDs to Pistachio')\n",
    "df['quadrant'] = cell_ids\n",
    "print('Finished Calculations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f670be-e5c2-4795-9722-066b01b36f7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(df, open('RSFP_Train_Expert_df.pkl','wb')) # Saving the dataset with expert ID for LLM trainigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2000a6e2-e95e-45f7-95d0-6ac46f77b004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_index(cpu_index, \"RSFP_Index.index\") # saving index to be loaded next time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135af618-4d6c-4047-87b8-24035c10c005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the code below to test\n",
    "index = read_index(\"RSFP_Index.index\") # testing reading the file\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "\n",
    "df = pickle.load(open('RSFP_Train_Expert_df.pkl','rb'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DMS_Env",
   "language": "python",
   "name": "paroute_cross_referencing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
